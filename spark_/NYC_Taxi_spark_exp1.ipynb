{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/spark/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.13 :: Continuum Analytics, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x108753c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure SparkContext is loaded \n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library \n",
    "import pandas as pd, numpy as np\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load applications  \n",
    "conf = SparkConf().setAppName(\"building a warehouse\")\n",
    "#sc = SparkContext(conf=conf)\n",
    "sqlCtx = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'NYC_Taxi_Trip_Duration/spark_/'\n",
      "/Users/yennanliu/notebook\n"
     ]
    }
   ],
   "source": [
    "cd NYC_Taxi_Trip_Duration/spark_/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read csv via spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv via spark SQL  \n",
    "df_train = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "\t\t\t\t\t\t .options(header='true', inferschema='true')\\\n",
    "\t\t\t\t\t\t .load('/Users/yennanliu/NYC_Taxi_Trip_Duration/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('vendor_id', 'int'),\n",
       " ('pickup_datetime', 'timestamp'),\n",
       " ('dropoff_datetime', 'timestamp'),\n",
       " ('passenger_count', 'int'),\n",
       " ('pickup_longitude', 'double'),\n",
       " ('pickup_latitude', 'double'),\n",
       " ('dropoff_longitude', 'double'),\n",
       " ('dropoff_latitude', 'double'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('trip_duration', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data type\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values \n",
    "df_train.where( df_train['vendor_id'].isNull() ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|     pickup_datetime|    dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+--------------------+--------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id2875421|        2|2016-03-14 17:24:...|2016-03-14 17:32:...|              1| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|                 N|          455|\n",
      "|id2377394|        1|2016-06-12 00:43:...|2016-06-12 00:54:...|              1|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055|                 N|          663|\n",
      "|id3858529|        2|2016-01-19 11:35:...|2016-01-19 12:10:...|              1| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|                 N|         2124|\n",
      "|id3504673|        2|2016-04-06 19:32:...|2016-04-06 19:39:...|              1|-74.01004028320312|   40.719970703125|-74.01226806640625| 40.70671844482422|                 N|          429|\n",
      "|id2181028|        2|2016-03-26 13:30:...|2016-03-26 13:38:...|              1|-73.97305297851562|40.793209075927734| -73.9729232788086| 40.78252029418945|                 N|          435|\n",
      "|id0801584|        2|2016-01-30 22:01:...|2016-01-30 22:09:...|              6|-73.98285675048828| 40.74219512939453|-73.99208068847656|40.749183654785156|                 N|          443|\n",
      "|id1813257|        1|2016-06-17 22:34:...|2016-06-17 22:40:...|              4| -73.9690170288086| 40.75783920288086|-73.95740509033203| 40.76589584350586|                 N|          341|\n",
      "|id1324603|        2|2016-05-21 07:54:...|2016-05-21 08:20:...|              1|-73.96927642822266| 40.79777908325195|-73.92247009277344| 40.76055908203125|                 N|         1551|\n",
      "|id1301050|        1|2016-05-27 23:12:...|2016-05-27 23:16:...|              1|-73.99948120117188|40.738399505615234|-73.98578643798828| 40.73281478881836|                 N|          255|\n",
      "|id0012891|        2|2016-03-10 21:45:...|2016-03-10 22:05:...|              1|-73.98104858398438| 40.74433898925781| -73.9729995727539| 40.78998947143555|                 N|         1225|\n",
      "|id1436371|        2|2016-05-10 22:08:...|2016-05-10 22:29:...|              1|-73.98265075683594| 40.76383972167969|-74.00222778320312| 40.73299026489258|                 N|         1274|\n",
      "|id1299289|        2|2016-05-15 11:16:...|2016-05-15 11:34:...|              4|-73.99153137207031| 40.74943923950195|   -73.95654296875|  40.7706298828125|                 N|         1128|\n",
      "|id1187965|        2|2016-02-19 09:52:...|2016-02-19 10:11:...|              2|-73.96298217773438| 40.75667953491211|-73.98440551757812|40.760719299316406|                 N|         1114|\n",
      "|id0799785|        2|2016-06-01 20:58:...|2016-06-01 21:02:...|              1|-73.95630645751953|40.767940521240234|-73.96611022949219| 40.76300048828125|                 N|          260|\n",
      "|id2900608|        2|2016-05-27 00:43:...|2016-05-27 01:07:...|              1|-73.99219512939453| 40.72722625732422|-73.97465515136719|  40.7830696105957|                 N|         1414|\n",
      "|id3319787|        1|2016-05-16 15:29:...|2016-05-16 15:32:...|              1|-73.95551300048828|40.768592834472656|-73.94876098632812| 40.77154541015625|                 N|          211|\n",
      "|id3379579|        2|2016-04-11 17:29:...|2016-04-11 18:08:...|              1|-73.99116516113281| 40.75556182861328| -73.9992904663086|  40.7253532409668|                 N|         2316|\n",
      "|id1154431|        1|2016-04-14 08:48:...|2016-04-14 09:00:...|              1|-73.99425506591797| 40.74580383300781| -73.9996566772461| 40.72334289550781|                 N|          731|\n",
      "|id3552682|        1|2016-06-27 09:55:...|2016-06-27 10:17:...|              1|-74.00398254394531|  40.7130126953125|-73.97919464111328| 40.74992370605469|                 N|         1317|\n",
      "|id3390316|        2|2016-06-05 13:47:...|2016-06-05 13:51:...|              1|   -73.98388671875|40.738197326660156|-73.99120330810547| 40.72787094116211|                 N|          251|\n",
      "+---------+---------+--------------------+--------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_grp = df_train.groupBy('vendor_id')\n",
    "#df_grp.sum('pickup_longitude','dropoff_longitude').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'id2875421', vendor_id=2, pickup_datetime=datetime.datetime(2016, 3, 14, 17, 24, 55), dropoff_datetime=datetime.datetime(2016, 3, 14, 17, 32, 30), passenger_count=1, pickup_longitude=-73.9821548461914, pickup_latitude=40.76793670654297, dropoff_longitude=-73.96463012695312, dropoff_latitude=40.765602111816406, store_and_fwd_flag=u'N', trip_duration=455),\n",
       " Row(id=u'id2377394', vendor_id=1, pickup_datetime=datetime.datetime(2016, 6, 12, 0, 43, 35), dropoff_datetime=datetime.datetime(2016, 6, 12, 0, 54, 38), passenger_count=1, pickup_longitude=-73.98041534423828, pickup_latitude=40.738563537597656, dropoff_longitude=-73.99948120117188, dropoff_latitude=40.73115158081055, store_and_fwd_flag=u'N', trip_duration=663),\n",
       " Row(id=u'id3858529', vendor_id=2, pickup_datetime=datetime.datetime(2016, 1, 19, 11, 35, 24), dropoff_datetime=datetime.datetime(2016, 1, 19, 12, 10, 48), passenger_count=1, pickup_longitude=-73.9790267944336, pickup_latitude=40.763938903808594, dropoff_longitude=-74.00533294677734, dropoff_latitude=40.710086822509766, store_and_fwd_flag=u'N', trip_duration=2124),\n",
       " Row(id=u'id3504673', vendor_id=2, pickup_datetime=datetime.datetime(2016, 4, 6, 19, 32, 31), dropoff_datetime=datetime.datetime(2016, 4, 6, 19, 39, 40), passenger_count=1, pickup_longitude=-74.01004028320312, pickup_latitude=40.719970703125, dropoff_longitude=-74.01226806640625, dropoff_latitude=40.70671844482422, store_and_fwd_flag=u'N', trip_duration=429)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x108838210>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|       id|count(1)|\n",
      "+---------+--------+\n",
      "|id0515898|       1|\n",
      "|id2677357|       1|\n",
      "|id0556588|       1|\n",
      "|id1381256|       1|\n",
      "|id3524926|       1|\n",
      "|id0082224|       1|\n",
      "|id1631034|       1|\n",
      "|id3829159|       1|\n",
      "|id3048673|       1|\n",
      "|id0045059|       1|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.registerTempTable(\"df_train_table\")\n",
    "sqlContext.sql(\"\"\"\n",
    "                SELECT id, count(*) \n",
    "                FROM df_train_table\n",
    "                group by 1 \n",
    "                order by 2 desc \n",
    "                limit 10\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1458644|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) from df_train_table\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1) window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x10a016350>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = Window.partitionBy('id', 'vendor_id')\\\n",
    "               .orderBy('pickup_datetime')\\\n",
    "               .rowsBetween(-3, 3)\n",
    "\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<avg(passenger_count) OVER (PARTITION BY id, vendor_id ORDER BY pickup_datetime ASC ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_avg = mean(df_train['passenger_count']).over(window)\n",
    "moving_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'id0000015', vendor_id=1, pickup_datetime=datetime.datetime(2016, 5, 17, 9, 6, 59), dropoff_datetime=datetime.datetime(2016, 5, 17, 9, 39, 18), passenger_count=1, pickup_longitude=-73.98369598388672, pickup_latitude=40.780948638916016, dropoff_longitude=-73.95437622070312, dropoff_latitude=40.76417541503906, store_and_fwd_flag=u'N', trip_duration=1939, moving_avg=1.0),\n",
       " Row(id=u'id0000023', vendor_id=2, pickup_datetime=datetime.datetime(2016, 5, 28, 4, 34, 47), dropoff_datetime=datetime.datetime(2016, 5, 28, 5, 1, 47), passenger_count=1, pickup_longitude=-73.94206237792969, pickup_latitude=40.817779541015625, dropoff_longitude=-73.7889175415039, dropoff_latitude=40.64738845825195, store_and_fwd_flag=u'N', trip_duration=1620, moving_avg=1.0),\n",
       " Row(id=u'id0000250', vendor_id=1, pickup_datetime=datetime.datetime(2016, 3, 30, 8, 38, 35), dropoff_datetime=datetime.datetime(2016, 3, 30, 8, 46, 42), passenger_count=1, pickup_longitude=-73.99744415283203, pickup_latitude=40.7363395690918, dropoff_longitude=-73.98930358886719, dropoff_latitude=40.74203109741211, store_and_fwd_flag=u'N', trip_duration=487, moving_avg=1.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_window_ = df_train.withColumn('moving_avg', moving_avg)\n",
    "df_window_.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2) pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pickup_datetime: timestamp, passenger_count: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot1 = df_train.groupby('pickup_datetime')\\\n",
    "                    .pivot('id', values=['passenger_count'])\\\n",
    "                    .sum('dropoff_longitude')\n",
    "        \n",
    "df_pivot1                                                                                \n",
    "                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2016, 6, 30, 18, 23, 16), passenger_count=None),\n",
       " Row(pickup_datetime=datetime.datetime(2016, 4, 20, 11, 38, 30), passenger_count=None)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) basic functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__ = sc.textFile(\"/Users/yennanliu/NYC_Taxi_Trip_Duration/data/train.csv\")\n",
    "#df__.filter(lambda x: '2124' in x.).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'id,vendor_id,pickup_datetime,dropoff_datetime,passenger_count,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,store_and_fwd_flag,trip_duration',\n",
       " u'id2875421,2,2016-03-14 17:24:55,2016-03-14 17:32:30,1,-73.982154846191406,40.767936706542969,-73.964630126953125,40.765602111816406,N,455',\n",
       " u'id2377394,1,2016-06-12 00:43:35,2016-06-12 00:54:38,1,-73.980415344238281,40.738563537597656,-73.999481201171875,40.731151580810547,N,663',\n",
       " u'id3858529,2,2016-01-19 11:35:24,2016-01-19 12:10:48,1,-73.979026794433594,40.763938903808594,-74.005332946777344,40.710086822509766,N,2124',\n",
       " u'id3504673,2,2016-04-06 19:32:31,2016-04-06 19:39:40,1,-74.010040283203125,40.719970703125,-74.01226806640625,40.706718444824219,N,429']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df__.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('vendor_id', 'int'),\n",
       " ('pickup_datetime', 'timestamp'),\n",
       " ('dropoff_datetime', 'timestamp'),\n",
       " ('passenger_count', 'int'),\n",
       " ('pickup_longitude', 'double'),\n",
       " ('pickup_latitude', 'double'),\n",
       " ('dropoff_longitude', 'double'),\n",
       " ('dropoff_latitude', 'double'),\n",
       " ('store_and_fwd_flag', 'string'),\n",
       " ('trip_duration', 'int')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#protocols = df__.map(lambda x: x[4]).distinct()\n",
    "#protocols.take(10)\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = df_train.select('id','vendor_id','pickup_datetime').rdd\n",
    "#xx = df_train.select('vendor_id').rdd\n",
    "type(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-03-14', '2016-06-12', '2016-01-19', '2016-04-06']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date \n",
    "xx.map(lambda x: str(x[2]).split(\" \")[0] ).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'id2875421', 2), (u'id2377394', 1), (u'id3858529', 2), (u'id3504673', 2)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id and vendor id \n",
    "xx.map(lambda x: (x[0], x[1]) ).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter \n",
    "\n",
    "#xx.filter(lambda x : x[0] == 'id2875421').take(10)\n",
    "xx.filter(lambda x : x[0] == 'id2875421').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'id2875421', 2, datetime.datetime(2016, 3, 14, 17, 24, 55)),\n",
       " (u'id2377394', 1, datetime.datetime(2016, 6, 12, 0, 43, 35)),\n",
       " (u'id3858529', 2, datetime.datetime(2016, 1, 19, 11, 35, 24))]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.map(lambda x: (x[0:])).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'id2875421',\n",
       " 2,\n",
       " datetime.datetime(2016, 3, 14, 17, 24, 55),\n",
       " u'id2377394',\n",
       " 1,\n",
       " datetime.datetime(2016, 6, 12, 0, 43, 35),\n",
       " u'id3858529',\n",
       " 2,\n",
       " datetime.datetime(2016, 1, 19, 11, 35, 24),\n",
       " u'id3504673']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "\n",
    "xx.flatMap(lambda x: (x)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2016, 3, 14, 17, 24, 55),\n",
       " datetime.datetime(2016, 6, 12, 0, 43, 35),\n",
       " datetime.datetime(2016, 1, 19, 11, 35, 24),\n",
       " datetime.datetime(2016, 4, 6, 19, 32, 31)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xx\n",
    "xx.map(lambda x: (x[2]) ).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.map(lambda x: x[0] == 'id2875421').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD group \n",
    "# https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html#groupBy\n",
    "\n",
    "result = xx.groupBy(lambda x: x[1] % 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'id2875421', u'id2377394', u'id3858529']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.map(lambda x: x[0]).take(3)\n",
    "#sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'id2875421', 2, datetime.datetime(2016, 3, 14, 17, 24, 55)),\n",
       " (u'id2377394', 1, datetime.datetime(2016, 6, 12, 0, 43, 35)),\n",
       " (u'id3858529', 2, datetime.datetime(2016, 1, 19, 11, 35, 24))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.map(lambda x: (x[0], x[1],x[2]) ).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_key_2 = xx.map(lambda x: (x[0]) )\n",
    "#data_key_2.reduceByKey(lambda x: x + x).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'id0349415', 2), (u'id2017679', 1), (u'id1697645', 2), (u'id0429368', 2)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey\n",
    "\n",
    "#xx.reduceByKey(lambda x, y: x + y).collect()\n",
    "data_key_ = xx.map(lambda x: (x[0], x[1]) )\n",
    "data_key_.reduceByKey(lambda x, y: x + y).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, vendor_id: bigint, pickup_datetime: timestamp]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, vendor_id: bigint, pickup_datetime: timestamp]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd -> spark dataframe \n",
    "\n",
    "df_xx = sqlContext.createDataFrame(xx)\n",
    "df_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<id['show']>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xx['id'].show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|       id|count(1)|\n",
      "+---------+--------+\n",
      "|id3013319|       1|\n",
      "|id1622754|       1|\n",
      "|id2187774|       1|\n",
      "|id3921267|       1|\n",
      "|id2795297|       1|\n",
      "|id0130048|       1|\n",
      "|id2088360|       1|\n",
      "|id0454719|       1|\n",
      "|id2366364|       1|\n",
      "|id0187208|       1|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark dataframe to spark sql \n",
    "\n",
    "df_xx.registerTempTable(\"df_xx_table\")\n",
    "sqlContext.sql(\"\"\"\n",
    "                SELECT id, count(*) \n",
    "                FROM df_xx_table\n",
    "                group by 1 \n",
    "                order by 2 desc \n",
    "                limit 10\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'id2875421', vendor_id=2, pickup_datetime=datetime.datetime(2016, 3, 14, 17, 24, 55)),\n",
       " Row(id=u'id2377394', vendor_id=1, pickup_datetime=datetime.datetime(2016, 6, 12, 0, 43, 35)),\n",
       " Row(id=u'id3858529', vendor_id=2, pickup_datetime=datetime.datetime(2016, 1, 19, 11, 35, 24)),\n",
       " Row(id=u'id3504673', vendor_id=2, pickup_datetime=datetime.datetime(2016, 4, 6, 19, 32, 31)),\n",
       " Row(id=u'id2181028', vendor_id=2, pickup_datetime=datetime.datetime(2016, 3, 26, 13, 30, 55))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.map(lambda x: x[1]).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xx.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'id,vendor_id,pickup_da',\n",
       " u'id2875421,2,2016-03-14',\n",
       " u'id2377394,1,2016-06-12',\n",
       " u'id3858529,2,2016-01-19',\n",
       " u'id3504673,2,2016-04-06',\n",
       " u'id2181028,2,2016-03-26',\n",
       " u'id0801584,2,2016-01-30',\n",
       " u'id1813257,1,2016-06-17',\n",
       " u'id1324603,2,2016-05-21',\n",
       " u'id1301050,1,2016-05-27']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df__.map(lambda x: x[0:22]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df__.map(lambda x: x[12:22]).take(10)\n",
    "\n",
    "#df__value = df__.filter(lambda line: line != header)\n",
    "#result = df__value.groupBy(lambda x : x[0:][12:22]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2016-03-14', u'2016-06-12', u'2016-01-19', u'2016-04-06', u'2016-03-26', u'2016-01-30', u'2016-06-17', u'2016-05-21', u'2016-05-27', u'2016-03-10']\n"
     ]
    }
   ],
   "source": [
    "header = df__.first()\n",
    "df__value = df__.filter(lambda line: line != header)\n",
    "df__value.map(lambda x : x[0:][12:22]).take(10)\n",
    "print (df__value.map(lambda x : x[0:][12:22]).take(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.countByKey().items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3),\n",
       " ('fleece', 7),\n",
       " ('had', 2),\n",
       " ('lamb', 5),\n",
       " ('little', 4),\n",
       " ('Mary', 1),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().first()\n",
    "sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
    "sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize(range(0,5))\n",
    "y = sc.parallelize(range(1000, 1005))\n",
    "x.zip(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
